\documentclass[12pt]{article}

\usepackage{ulem}
\usepackage{multicol}

\begin{document}

\title{\vfill Survey Paper Proposal: Web Application Caching}

\author{
By Marc Zych \vspace{10pt} \\
CSC 560: Grad Databases \vspace{10pt} \\
Dr. Alexander Dekhtyar \vspace{10pt} \\
}
\date{\today}

\maketitle

% \begin{abstract}
% Stuff
% \end{abstract}

\thispagestyle{empty}
\newpage

%\tableofcontents

%\newpage
%\begin{multicols}{2}

\section{Related Work}
Web scalability has been an issue for web sites since the dot-com boom of the late nineties.
All moderately sized websites experience these issues as their breadth of content as well as the number of active users grow.
The ubiquitous solution to this problem is some form of caching.
Although most web servers employ generous amounts of caching, the methods and approaches used vary widely.

The remainder of this section is divided into different topics of research that revolve around improving the effectiveness of caching.

\subsection{Incorporating Cache}
\subsubsection{Caching Technologies}
Most web application caching research revolves around caching strategies.
It turns out that the underlying cache daemon is largely irrelevant.
Memcached is by far the most popular web caching system; it is currently deployed by Facebook, Wikipedia, YouTube, and countless others.\cite{memcachedDotOrg}

The exception to this trend is a paper that proposes an alternative called SQLCached.\cite{sqlCached}
This tool offers a few things that memcached doesn't: complex data types and rich querying for retrieval and invalidation.
However, memcached is a more general purpose caching system that has many benefits including O(1) operations and inherently distributed nature.\cite{memcachedDotOrg}

Memcached is an example of a dynamic cache daemon.
There are also static caches that are only updated periodically and are not modified directly by the web application.
Static caches are popular for search engines because a large number of search queries' return the same data for long amounts of time.
These queries can be determined by analyzing historical data which would be difficult to do using a dynamic cache.\cite{designTradeOffsSearchEngine}
In particular, the knapsack problem can be used to fill the static cache if the request frequency and size of cacheable items can be determined.\cite{designTradeOffsSearchEngine}
Additionally, many papers propose a 2 or 3-level cache that incorporates both static and dynamic caches.\cite{cacheAdmissionPolicies}\cite{designTradeOffsSearchEngine}

\subsubsection{Implementing in an Application}
Many approaches have been proposed for how to implement caching strategies in web application code.
The most straight forward method is to explicitly get, set, and invalidate caches wherever they are needed in the application.
However, this is tedious and error-prone.\cite{keyBasedCacheExpiration}\cite{triggerBasedORM}
A preferrable alternative is to have caching built directly into the underlying application code.
For example, Ruby on Rails provides a very rich data model that handles most of the caching details behind the scenes.\cite{keyBasedCacheExpiration}

Another Object Relational Model (ORM) has been developed for the Django web framework that automatically handles all necessary caching operations which leaves the developer to write application code.\cite{triggerBasedORM}
The project, CacheGenie, provides Python callbacks to database triggers that will be called when queries are run on the database. This allows the tool to update and/or invalidate caches depending on the queries.
The major advantage of this approach is that no matter how the data is changed the cache remains consistent because cache invalidation occurs at the data layer.
Other projects have used database triggers with similar results.\cite{scalableConsistentCaching}

\subsection{Determining what to cache}
Some research has been done looking into what data is most efficiently cached.
Many sources propose that fragments of HTML are the most useful data to cache.\cite{comparisonOfCachingSolutions}\cite{scalableConsistentCaching}
A cache hit for HTML allows the server to avoid generating the HTML as well as retrieving the underlying data from the persistent store.\cite{howBasecampGotSoFast}

One paper proposed that enforcing cache admission policies was a valuable way to improve hit ratio.\cite{cacheAdmissionPolicies}
The driving idea behind it is that there are many singleton queries (queries that are run only once) that pollute the cache because they are not used again.
Detecting such queries and leaving them out of the cache leaves more room for frequently accessed queries thus increasing cache hit ratio.
However, another paper claimed that the eviction policy doesn't have a significant impact on cache hit ratio.\cite{refreshingPerspectiveSearch}
This is because caches can easily be expanded to hold more entries and therefore items rarely get evicted to begin with.\cite{refreshingPerspectiveSearch}

\subsection{Cache Consistency}
The most difficult problem regarding caches is ensuring consistency with the underlying persistent data store.
The largest body of research for web caching focuses on search engines because of their monumental data set, request rate, and constantly changing data.

\subsubsection{Timestamps}
One paper proposed a system that does not invalidate caches but rather determines cache freshness upon cache hit.\cite{cacheInvalidationWebSearch}
This is done by including a timestamp corresponding to the generation time in the cached object.
Upon cache hit the timestamp is compared against a query index that is updated as the underlying data is updated.

Another caching technique is referred to as key based cache expiration\cite{keyBasedCacheExpiration} or generational caching\cite{generationalCaching}.
The idea is that a timestamp corresponding to the last time the object was updated is included in the key for any caches that depend on it.
The timestamp is updated once the object is modified thus invalidating all necessary keys.
This approach also updates the timestamps of any objects that depend on the modified object.
This technique is employed by 37Signals and various other websites.\cite{keyBasedCacheExpiration}

\subsubsection{Write-Through Caching}
Write-through caching is a technique that involves saving the data in the cache at the same as it is being saved in the persistent store.\cite{writeThroughCaching}
This technique improves cache hit rate because cache misses will only happen as a result of cached values being evicted or a server failure.
However, this technique only works for simple data types.
Any other objects that depend on the current object being saved can't be updated in the cache because that data isn't directly accessible and will require a database lookup.

\subsubsection{Prefetching}
Prefetching content is another common solution for caching.
This technique involves caching content before it is requested so it can be retrieved quickly when it is needed.
Generally an event triggers the precaching system to intelligently decide what content to prefetch in the hopes that the data will be requested soon.
This takes advantage of spatial locality.

A caching system was developed that addresses the problem of cache stampedes.\cite{scalableConsistentCaching}
A cache stampede occurs when a cache entry is invalidated and the content is regenerated multiple times.
This results in a large spike in database load because there are many requests for the same content.
This can be avoided by employing a prefetching and write-through caching mechanism.
Rather than invalidating any necessary caches, the content can be prefetched and the cache's value will be updated in place.
This approach avoids cache misses at the cost of slight data staleness because the old copy will still be served while the fresh copy is being prefetched.
The website for the 1998 Winter Olympic games deployed this caching technique and reached nearly 100\% cache hits.\cite{scalableConsistentCaching}

\subsubsection{Dependency graph}
One paper describes a system that constructs a dependency graph that is used when invalidating caches.\cite{scalableConsistentCaching}
When an object is updated all of the caches that have an edge with that object are invalidated along with the original object.
This approach is similar to using timestamps in that the objects whose timestamps are updated when another object is updated represent an edge in a dependency graph.

\subsubsection{Expiration times}
Another paper exploits the cache expiration time to invalidate caches.\cite{refreshingPerspectiveSearch}
The idea is that serving slightly stale data isn't terrible for search engines.
Additionally, the rate of change can be determined for a given query and can be factored into the expiration time.
This allows frequently changing data to have a shorter time to live and infrequently changing data to have a longer time to live.

\subsection{Eviction Policies}
Despite what \cite{refreshingPerspectiveSearch} claims, research has been done investigating eviction policies.
Least recently used (LRU) is a commonly used eviction policy from operating systems to CPU caches to dynamic web data.
There has been research into least likely to be used (LLU) which is an optimal policy that can be approximated by various techniques.

\section{Validation}
\subsection{General Approach}
Validating my work is a fairly straightforward process.
The basic approach will involve gathering various statistics with and without my approach during an experiment based on a repeatable set of input data. %TODO: This sentence kinda sucks.

\subsection{Measured Values}
Some values that should be increased include cache hit rate and requests per second.
Some values that should be decreased include server load and page response time.
However, merely comparing the two sets of data will not be an accurate picture of the situation.
For example, cache hit rate may be increased as well as server load.
This could be the result of an optimistic prefetching algorithm that caches values before they are requested in the event that they will be requested soon.
The pivotal question is regarding whether or not the increased server load is worth the increased hit rate.

Other criteria, such as ease of use and cache freshness, are harder to measure and validate.
Cache freshness involves measuring how long a stale cache is served after there is a fresh version available.
In particular, some amount of cache staleness is acceptable but the line is blurred especially in the presence of performance gains.

Ease of use is even more difficult to measure.
A caching mechanism that requires a diligent programmer to carefully hand craft the caching algorithm with every feature added is not very easy to use.
On the other hand, a caching mechanism that operates behind the scenes without the programmer's knowledge is very easy to use.
Measuring this will be a very subjective matter.

\subsection{Experimental Setup}
Developing an experiment to get consistent results is crucial.
I am planning on recording all site traffic during a period of time and replaying it during the experiments.
This ensures that the workload will be the same across all tests.

First an unaltered log of site traffic will be replayed on both systems to emulate real world traffic patterns.
Next this log will be extrapolated to simulate increased site traffic.
Multiple levels of increased site traffic will be run on both systems as a comparison of their merits of scalability.
Scalability tests can be split into two separate groups: general traffic and targetted traffic.
General traffic isn't targetted to any specific pages; requests to all pages are uniformly increased.
This is similar to increasing a site's general popularity.
Targetted traffic on the other hand has increased traffic to a small subset of content.
This simulates special events such as press and social media where a page goes viral for a short amount of time.
Both types of traffic are realistic and are useful measures for the effectiveness of caching strategies.

The contents of the database and memcached must be the same at the start of the experiments.
The contents of memcached can be flushed to ensure a consistent repeatable experiment.
However, this setup isn't very realistic because the contents of cache is never completely empty in production and may skew results depending on differences in the caching algorithms.
To mitigate this issue, experiments can be run for a period of time before statistics are collected to make the contents of cache more realistic.

Ensuring the database is in a consistent state is a more difficult matter.
Simply performing a dump of the database won't work because of how long it takes to do so.
Instead, a slave database can be set up to be replicated from the master database.
The instant site traffic starts being recorded the slave will stop replication and the dump will be performed on the slave database.
The contents of the test database will be populated with this dump to guarantee a repeatable experiment.
The only problem with this approach is that populating a database from a MySQL dump may take a significant amount of tme depending on the size of the data.
This will merely increase the time between tests.

All experiments will be run on a duplicate set of machines that mirror the architecture that is used in production.
This ensures that the results gathered will be consistent with how they will perform in production.

%\end{multicols}

\newpage
\nocite{*}
\bibliographystyle{IEEEannot}
\bibliography{memcache-survey}
\end{document}
