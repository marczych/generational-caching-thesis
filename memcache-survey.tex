\documentclass[12pt]{article}

\usepackage{ulem}
\usepackage{multicol}

\begin{document}

\title{\vfill Survey Paper: Web Application Caching}

\author{
By Marc Zych \vspace{10pt} \\
CSC 560: Grad Databases \vspace{10pt} \\
Dr. Alexander Dekhtyar \vspace{10pt} \\
}
\date{\today}

\maketitle

% \begin{abstract}
% Stuff
% \end{abstract}

\thispagestyle{empty}
\newpage

%\tableofcontents

%\newpage
%\begin{multicols}{2}

\section{Introduction}
Web scalability has been an issue for web sites since the dot-com boom of the late nineties.
All moderately sized websites experience these issues as their breadth of content as well as the number of active users grow.
The ubiquitous solution to this problem is some form of caching.

Although most web servers employ generous amounts of caching, the methods and approaches used vary widely.
Client side browser caching is popular because it avoids a large amount of network overhead.
This is typically done by determining whether or not the content has changed since it was last requested.
Proxy caching is very similar to browser caching except that it happens at proxy servers located in between the client and the primary host server.
This survey, however, focuses on caching strategies and techniques in web application code that are aimed to reduce database load and decrease page response time.

The remainder of this paper is structured as follows.
Section 2 introduces caching terminology, section 3 describes various challenges that caching research focusses on, sections 4, 5, and 6 explain in detail the content of the papers surveyed, and finally section 7 concludes.

\section{Terminology}
This section defines some of the terminology used in the field of caching and the remainder of this paper.
At a high level a cache is used to store results of operations so future requests can be served faster.
A cache is typically not the primary storage location for data.
In most cases there is a permanent backing store such as a database.
A cache hit occurs when the data that is being requested is in the cache and no further action is required to retrieve the data.
A cache miss occurs when the data that is being requested is not in the cache and it must be retrieved from the permanent backing store.

Caching is effective because retrieving data from the cache is much faster than retrieving it from its persistent location.
Caching also exploits locality.
Temporal locality is the idea that an item that has been referenced recently will likely be referenced again in the future.
Spatial locality is the idea that an item that is next to an item getting requested is likely to be referened in the future.
Traditionally this meant physically next to the object such as in neighboring memory locations.
However, for web sites it has been broadened to include things like recommended content.

Caches are typically fairly small in comparison to other persistent stores.
Caches come in two main flavors: static and dynamic. %TODO This sentence sucks.
A static cache is one that is not updated as a result of the application running.
The application can request data from the cache but it cannot add, modify, or remove cache entries.
However, the cache is updated periodically to keep it up to date.
The timeframe for this differs from application to application but is largely a function of how often the underlying data changes.
In contrast, a dynamic cache is one that can be updated as a result of the application execution.
The application can freely add, modify, and remove cache entries based on the operations it is performing.

Dynamic caches have a limited size and may eventually fill to capacity
When this occurs items must be removed to make room for new ones.
The eviction policy determines which cache entries are removed.
Some caches allow entries to be set with an expire time.
After the specified time has passed the entry is expired and will no longer be returned by the cache.

Cache coherence, also referred to as cache consistency, is the concept of keeping the data in the cache consistent with the data stored in the primary backing store.
One such method is to perform cache invalidation which removes items from the cache.
A cache stampede can occur when a cache that is frequently requested is invalidated.
There are then a series of cache misses followed by a series of duplicated requests to the backing store followed by a series of cache sets.
These should be avoided if possible because of the redundant work done by each request.

\section{Research Challenges}
Most research is focussed on addressing two core challenges with caching.
The first of which is cache coherency.
In some cases maintaining cache coherency is trivial because there is only one cache associated with the underlying data.
However, with more complex data the situation is much more difficult.
There may be dependencies between objects which means that multiple caches need to be invalidated at the same time.
To further complicate things the dependencies between objects may be dynamic and must be resolved at run time on a per object basis.
This is very common for Web 2.0 sites that are driven by user generated content.

Another research challenge is how to make the caching system easy to use for developers.
The problem is that manually controlling the cache is tedious and error-prone.
The idea is that this can be remedied by putting in place a system that transparently handles all caching operations which leaves the developer to write the more meaningful code.

Core area of research is search engines. %TODO Add this or not?

\section{Incorporating Cache}
\subsection{Caching Technologies}
Most web application caching research revolves around caching strategies.
It turns out that the underlying cache daemon is largely irrelevant.
Memcached is by far the most popular web caching system; it is currently deployed by Facebook, Wikipedia, YouTube, and countless others.\cite{memcachedDotOrg}

The exception to this trend is a paper that proposes an alternative called SQLCached.\cite{sqlCached}
This tool offers a few things that memcached doesn't: complex data types and rich querying for retrieval and invalidation.
However, memcached is a more general purpose caching system that has many benefits including O(1) operations and inherently distributed nature.\cite{memcachedDotOrg}

Memcached is an example of a dynamic cache daemon.
There are also static caches that are only updated periodically and are not modified directly by the web application.
Static caches are popular for search engines because a large number of search queries' return the same data for long amounts of time.
These queries can be determined by analyzing historical data which would be difficult to do using a dynamic cache.\cite{designTradeOffsSearchEngine}
In particular, the knapsack problem can be used to fill the static cache if the request frequency and size of cacheable items can be determined.\cite{designTradeOffsSearchEngine}
Additionally, many papers propose a 2 or 3-level cache that incorporates both static and dynamic caches.\cite{cacheAdmissionPolicies}\cite{designTradeOffsSearchEngine}

\subsection{Implementing in an Application}
Many approaches have been proposed for how to implement caching strategies in web application code.
The most straight forward method is to explicitly get, set, and invalidate caches wherever they are needed in the application.
However, this is tedious and error-prone.\cite{keyBasedCacheExpiration}\cite{triggerBasedORM}
A preferrable alternative is to have caching built directly into the underlying application code.
For example, Ruby on Rails provides a very rich data model that handles most of the caching details behind the scenes.\cite{keyBasedCacheExpiration}

Another Object Relational Model (ORM) has been developed for the Django web framework that automatically handles all necessary caching operations which leaves the developer to write application code.\cite{triggerBasedORM}
The project, CacheGenie, provides Python callbacks to database triggers that will be called when queries are run on the database. This allows the tool to update and/or invalidate caches depending on the queries.
The major advantage of this approach is that no matter how the data is changed the cache remains consistent because cache invalidation occurs at the data layer.
Other projects have used database triggers with similar results.\cite{scalableConsistentCaching}

\section{Cache Consistency}
The most difficult problem regarding caches is ensuring consistency with the underlying persistent data store.
The largest body of research for web caching focuses on search engines because of their monumental data set, request rate, and constantly changing data.

\subsection{Timestamps}
One paper proposed a system that does not invalidate caches but rather determines cache freshness upon cache hit.\cite{cacheInvalidationWebSearch}
This is done by including a timestamp corresponding to the generation time in the cached object.
Upon cache hit the timestamp is compared against a query index that is updated as the underlying data is updated.

Another caching technique is referred to as key based cache expiration\cite{keyBasedCacheExpiration} or generational caching\cite{generationalCaching}.
The idea is that a timestamp corresponding to the last time the object was updated is included in the key for any caches that depend on it.
The timestamp is updated once the object is modified thus invalidating all necessary keys.
This approach also updates the timestamps of any objects that depend on the modified object.
This technique is employed by 37Signals and various other websites.\cite{keyBasedCacheExpiration}

\subsection{Write-Through Caching}
Write-through caching is a technique that involves saving the data in the cache at the same as it is being saved in the persistent store.\cite{writeThroughCaching}
This technique improves cache hit rate because cache misses will only happen as a result of cached values being evicted or a server failure.
However, this technique only works for simple data types.
Any other objects that depend on the current object being saved can't be updated in the cache because that data isn't directly accessible and will require a database lookup.

\subsection{Prefetching}
Prefetching content is another common solution for caching.
This technique involves caching content before it is requested so it can be retrieved quickly when it is needed.
Generally an event triggers the precaching system to intelligently decide what content to prefetch in the hopes that the data will be requested soon.
This takes advantage of spatial locality.

A caching system was developed that addresses the problem of cache stampedes.\cite{scalableConsistentCaching}
A cache stampede occurs when a cache entry is invalidated and the content is regenerated multiple times.
This results in a large spike in database load because there are many requests for the same content.
This can be avoided by employing a prefetching and write-through caching mechanism.
Rather than invalidating any necessary caches, the content can be prefetched and the cache's value will be updated in place.
This approach avoids cache misses at the cost of slight data staleness because the old copy will still be served while the fresh copy is being prefetched.
The website for the 1998 Winter Olympic games deployed this caching technique and reached nearly 100\% cache hits.\cite{scalableConsistentCaching}

\subsection{Dependency graph}
One paper describes a system that constructs a dependency graph that is used when invalidating caches.\cite{scalableConsistentCaching}
When an object is updated all of the caches that have an edge with that object are invalidated along with the original object.
This approach is similar to using timestamps in that the objects whose timestamps are updated when another object is updated represent an edge in a dependency graph.

\subsection{Expiration times}
Another paper exploits the cache expiration time to invalidate caches.\cite{refreshingPerspectiveSearch}
The idea is that serving slightly stale data isn't terrible for search engines.
Additionally, the rate of change can be determined for a given query and can be factored into the expiration time.
This allows frequently changing data to have a shorter time to live and infrequently changing data to have a longer time to live.

\section{Determining what to cache}
Some research has been done looking into what data is most efficiently cached.
Many sources propose that fragments of HTML are the most useful data to cache.\cite{comparisonOfCachingSolutions}\cite{scalableConsistentCaching}
A cache hit for HTML allows the server to avoid generating the HTML as well as retrieving the underlying data from the persistent store.\cite{howBasecampGotSoFast}

One paper proposed that enforcing cache admission policies was a valuable way to improve hit ratio.\cite{cacheAdmissionPolicies}
The driving idea behind it is that there are many singleton queries (queries that are run only once) that pollute the cache because they are not used again.
Detecting such queries and leaving them out of the cache leaves more room for frequently accessed queries thus increasing cache hit ratio.
However, another paper claimed that the eviction policy doesn't have a significant impact on cache hit ratio.\cite{refreshingPerspectiveSearch}
This is because caches can easily be expanded to hold more entries and therefore items rarely get evicted to begin with.\cite{refreshingPerspectiveSearch}

\subsection{Eviction Policies}
Despite what \cite{refreshingPerspectiveSearch} claims, research has been done investigating eviction policies.
Least recently used (LRU) is a commonly used eviction policy from operating systems to CPU caches to dynamic web data.
There has been research into least likely to be used (LLU) which is an optimal policy that can be approximated by various techniques.

\section{Conclusion}
Conclude!

% \subsection{Locality}
% All computer caches work by exploiting spatial and temporal locality.
% Research has been done looking into maximizing these traits for web application caching.\cite{onCachingSearchEngineResults}\cite{cacheAdmissionPolicies}

% \section{Summaries}
% triggerBasedORM \cite{triggerBasedORM}
% Proposes CacheGenie: a caching middleware which makes it easy for web application developers to use caching mechanisms in their applications.
% Developer doesn't worry about cache invalidation while writing application code.
% Uses memcached.
% Uses database triggers to update/invalidate cache.
% Various consistency schemes.
% Caching lists and incrementally updating contents.

% sqlCached \cite{sqlCached}
% Didn't like Memcached so they wrote their own caching thing using sqllite.
% Specifically wanted rich querying and complex data.

% cacheInvalidationWebSearch \cite{cacheInvalidationWebSearch}
% Web search caching.
% Online cache invalidation.
% Query-driven cache invalidationf ramework - invalidation devision occurs on cache hit which avoids redundant invalidations.
% Use generation time of cached queries.
% Handles additions, updates, and deletions.

% refreshingPerspectiveSearch \cite{refreshingPerspectiveSearch}
% Eviction policies are not the issue because of the size of caches - freshness is.
% Use a TTL and let it expire - easy but not very good.
% Propose a strategy that refreshes caches, the time being dynamic - this is in addition to TTLs.

% cacheAdmissionPolicies \cite{cacheAdmissionPolicies}
% Proposes admission policies to prevent infrequent or even singleton queries from polluting the cache.
% Temporal locality.
% Singleton queries are single results that never occur again and increases cache misses because of evictions.
% Propose optimal admission policy over LRU that uses a heuristic to separate infrequent queries from frequent ones.
% Two stage cache - one for admission regulated queries and another for all other ones using LRU.
% Uses previous data to determine important ones.

% designTradOffs \cite{designTradeOffsSearchEngine}
% Studies trade-offs in static vs. dynamic caching, caching query results vs. caching posting lists.
% Correlates to the knapsack problem (optimal packing) for static cache.
% You know frequencies and sizes beforehand and can take a long time to come up with the best set to cache.

% keyBasedCacheExpiration \cite{keyBasedCacheExpiration}
% Generational caching - use last modified as part of the cache key so you don't ever have to explicitly delete anything.
% Automatically invalidates all of your caches.

% comparisonOfCachingSolutions \cite{comparisonOfCachingSolutions}
% Caching dynamic pages is difficult so do fragments of HTML instead.
% Proposes Dynamic Content Acceleration solution which does just that.
% Mentions observation-based invalidation.

% timestamp \cite{timestampCacheInvalidation}
% Include last updated timestamp for queries and predict when the data is stale.
% Predict when items are stale and update them accordingly.

% onCachingSerachEngineREsults \cite{onCachingSearchEngineResults}
% Investigates replacement policies.
% Looks at static caching.
% Locality is key.

% scalableConsistentCaching \cite{scalableConsistentCaching}
% Data Update Propagation system that maintains dependence information between cached objects and their underlying data.
% Uses graph traversals to do stuff.
% Cache fragments of HTML.
% Object dependence graph to represent dependencies.
% Prefetching pages -> generate new content immediately and replace cache on top so every page load is a cache hit.
% Use database triggers to update values used in caching.
% Avoid cache stampedes by prefetching and updating caches in place (don't invalidate).

% cachingdynamicWebContent \cite{cachingDynamicWebContent}
% Fine grained invalidation is essential
% Parse SQL queries to invalidate caches

%\end{multicols}

\newpage
\nocite{*}
\bibliographystyle{IEEEannot}
\bibliography{memcache-survey}
\end{document}
