
% Cal Poly Thesis
% 
% based on UC Thesis format
%
% modified by Mark Barry 2/07.
%


\documentclass[12pt]{ucthesis}

\usepackage{url}

\usepackage[pdftex]{graphicx}
% Update title and author below...
\usepackage[pdftex,plainpages=false,breaklinks=true,colorlinks=true,urlcolor=blue,citecolor=blue,%
                                   linkcolor=blue,bookmarks=true,bookmarksopen=true,%
                                   bookmarksopenlevel=3,pdfstartview=FitV,
                                   pdfauthor={!!Author goes here!!},
                                   pdftitle={!!Title goes here!!},
                                   pdfkeywords={thesis, masters, cal poly}
                                   ]{hyperref}
%Options with pdfstartview are FitV, FitB and FitH
\pdfcompresslevel=1

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[letterpaper]{geometry}
\usepackage[overload]{textcase}

\bibliographystyle{abbrv}

\setlength{\parindent}{0.25in} \setlength{\parskip}{6pt}

\geometry{verbose,nohead,tmargin=1.25in,bmargin=1in,lmargin=1.5in,rmargin=1.3in}

\setcounter{tocdepth}{2}


% Different font in captions (single-spaced, bold) ------------
\newcommand{\captionfonts}{\small\bf\ssp}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter
% ---------------------------------------




\begin{document}

% Declarations for Front Matter

%TODO Update fields below!
\title{This is the Title of My Thesis}
\author{Marc Zych}
\degreemonth{February} \degreeyear{2009} \degree{Master of Science}
\defensemonth{February} \defenseyear{2009}
\numberofmembers{3} \chair{Zo\"{e} Wood, Ph.D.} \othermemberA{Chris Clark, Ph.D.} \othermemberB{Chris Buckalew, Ph.D.} \field{Computer Science} \campus{San Luis Obispo}
\copyrightyears{seven}



\maketitle

\begin{frontmatter}

% Custom made for Cal Poly (by Mark Barry, modified by Andrew Tsui).
\copyrightpage

% Custom made for Cal Poly (by Andrew Tsui).
\committeemembershippage

\begin{abstract}

Caching is hard. I'm going to make it better.

\end{abstract}


\tableofcontents


\listoftables

\listoffigures

\end{frontmatter}

\pagestyle{plain}




\renewcommand{\baselinestretch}{1.66}


\chapter{Introduction}
\label{introduction}

\section{Motivation}
Web scalability has been an issue for web sites since the dot-com boom of the late nineties.
All moderately sized websites experience these issues as their breadth of content as well as the number of active users grow.
The ubiquitous solution to this problem is some form of caching.

Although most web servers employ generous amounts of caching, the methods and approaches used vary widely.
Client side browser caching is popular because it avoids a large amount of network overhead.
This is typically done by determining whether or not the content has changed since it was last requested.
Proxy caching is very similar to browser caching except that it happens at proxy servers located in between the client and the primary host server.
This survey, however, focuses on caching strategies and techniques in web application code that are aimed to reduce database load and decrease page response time.


\subsection{General}
General.

\subsection{iFixit}
iFixit.

\subsection{Caching}
Caching.

\section{Main Contributions}
Main contributions.

The rest of this thesis is organized as follows...


\chapter{Background}
\label{background}

\section{More iFixit Background}
More iFixit background.

\section{Caching}
Caching.

\section{How iFixit Caches Now}
How iFixit caches now.

\section{Related Work}

\subsection{Terminology}
At a high level a cache is used to store results of operations so future requests can be served faster.
A cache is typically not the primary storage location for data.
In most cases there is a permanent backing store such as a database which is used to retrieve information and populate the cache.
A cache hit occurs when the data that is being requested is in the cache and no further action is required.
A cache miss occurs when the data that is being requested is not in the cache and it must be retrieved from elsewhere.

Caching is effective because retrieving data from the cache is much faster than retrieving it from its persistent location.
Additionally, caching exploits locality.
Temporal locality is the idea that an item that has been referenced recently will likely be referenced again in the future.
Spatial locality is the idea that an item that is next to an item getting requested is likely to be referenced in the future.
Traditionally this meant physically next to the object such as in neighboring memory locations.
However, for web sites it has been broadened to include things like recommended content.

Caches can be of the static or dynamic type.
A static cache is one that is not updated as a result of the application running.
The application can request data from the cache but it cannot add, modify, or remove cache entries.
However, the cache is updated periodically to keep it up to date.
The timeframe for this differs from application to application but is largely a function of how often the underlying data changes.
In contrast, a dynamic cache is one that can be updated as a result of the application execution.
The application can freely add, modify, and remove cache entries based on the operations it is performing.

Dynamic caches have a limited size and may eventually fill to capacity
When this occurs items must be removed to make room for new ones.
The eviction policy determines which cache entries are removed.
Some caches allow entries to be set with an expire time.
After the specified time has passed the entry is expired and is no longer a valid cache entry.

Cache coherence, also referred to as cache consistency, is the concept of keeping the data in the cache consistent with the data stored in the primary backing store.
One such method is to perform cache invalidation which removes items from the cache.
A cache stampede can occur when a cache that is frequently requested is invalidated.
There are then a series of cache misses followed by a series of duplicated requests to the backing store followed by a series of cache sets.
These should be avoided if possible because of the redundant work done by each request.

\subsection{Research Challenges}
Most research is focused on addressing two core challenges with caching.
The first of which is cache coherency.
In some cases maintaining cache coherency is trivial because there is only one cache associated with the underlying data.
However, with more complex data the situation is much more difficult.
There may be dependencies between objects which means that multiple caches need to be invalidated at the same time.
To further complicate things the dependencies between objects may be dynamic and must be resolved at run time on a per object basis.
This is very common for Web 2.0 sites that are driven by user generated content.

Another research challenge is how to make the caching system easy to use for developers.
The problem is that manually controlling the cache is tedious and error-prone.
The idea is that this can be remedied by putting in place a system that transparently handles all caching operations which leaves the developer to write the more meaningful code.

\subsection{Incorporating Cache}
\subsubsection{Caching Technologies}
Most web application caching research revolves around caching strategies.
It turns out that the underlying cache daemon is largely irrelevant.
Memcached is by far the most popular web caching system; it is currently deployed by Facebook, Wikipedia, YouTube, and countless others.\cite{memcachedDotOrg}

The exception to this trend is a paper that proposes an alternative called SQLCached.\cite{sqlCached}
This tool offers a few things that memcached doesn't: complex data types and rich querying for retrieval and invalidation.
This is possible because SQLCached uses SQLite as its caching mechanism.
However, memcached is a more general purpose caching system that has many benefits including constant time ({\tt O(1)}) operations and inherently distributed nature.\cite{memcachedDotOrg}

Memcached is an example of a dynamic cache daemon.
There are also static caches that are only updated periodically and are not modified directly by the web application.
Static caches are popular for search engines because a large number of search queries return the same data for long periods of time.
These queries can be determined by analyzing historical data which would be difficult to do using a dynamic cache.\cite{designTradeOffsSearchEngine}
In particular, the knapsack problem can be used to fill the static cache if the request frequency and size of cacheable items can be determined.\cite{designTradeOffsSearchEngine}
Additionally, many papers propose a 2 or 3-level cache that incorporates both static and dynamic caches.\cite{cacheAdmissionPolicies}\cite{designTradeOffsSearchEngine}

\subsubsection{Implementing in an Application}
Many approaches have been proposed for how to implement caching strategies in web application code.
The most straight forward method is to explicitly get, set, and invalidate caches wherever they are needed in the application.
However, this is tedious and error-prone.\cite{keyBasedCacheExpiration}\cite{triggerBasedORM}
A preferable alternative is to have caching built directly into the underlying application code.
For example, Ruby on Rails provides a very rich data model that handles most of the caching details behind the scenes.\cite{keyBasedCacheExpiration}

Another Object Relational Model (ORM) has been developed for the Django web framework that automatically handles all necessary caching operations.\cite{triggerBasedORM}
The project, CacheGenie, provides Python callbacks to database triggers that will be called when queries are run on the database. This allows the tool to update and/or invalidate caches depending on the queries.
The major advantage of this approach is that no matter how the data is changed the cache remains consistent because cache invalidation occurs at the data layer.
Other projects have used database triggers with similar results.\cite{scalableConsistentCaching}

\subsection{Cache Consistency}
\subsubsection{Write-Through Caching}
Write-through caching is a technique that involves saving the data in the cache at the same as it is being saved in the persistent store.\cite{writeThroughCaching}
This technique improves cache hit rate because cache misses will only happen as a result of cached values being evicted or a server failure.
However, this technique only works for simple data types.
Any other objects that depend on the current object being saved can't be updated in the cache because that data isn't directly accessible and will require a database lookup.

\subsubsection{Timestamps}
One paper proposed a system that does not invalidate caches but rather determines cache freshness upon cache hit.\cite{cacheInvalidationWebSearch}
This is done by including a timestamp corresponding to the generation time in the cached object.
Upon cache hit the timestamp is compared against a query index that is updated as the underlying data is updated.

Another caching technique is referred to as key based cache expiration\cite{keyBasedCacheExpiration} or generational caching\cite{generationalCaching}.
The idea is that a timestamp corresponding to the last time the object was updated is included in the key for any caches that depend on it.
The timestamp is updated once the object is modified thus invalidating all necessary keys.
This approach also updates the timestamps of any objects that depend on the modified object.
This technique is employed by 37Signals and various other websites.\cite{keyBasedCacheExpiration}

\subsubsection{Prefetching}
Prefetching content is another common solution for caching.
This technique involves caching content before it is requested so it can be retrieved quickly when it is needed.
Generally an event triggers the precaching system to intelligently decide what content to prefetch in the hopes that the data will be requested soon.
This approach takes advantage of spatial locality.

A caching system was developed that addresses the problem of cache stampedes.\cite{scalableConsistentCaching}
Rather than invalidating any necessary caches, the content can be prefetched and the cache's value will be updated in place using write-through caching.
This approach avoids cache misses at the cost of slight data staleness because the old copy will still be served while the fresh copy is being prefetched.
The website for the 1998 Winter Olympic games deployed this caching technique and reached nearly 100\% cache hits.\cite{scalableConsistentCaching}

\subsubsection{Dependency graph}
One paper describes a system that constructs a dependency graph that is used when invalidating caches.\cite{scalableConsistentCaching}
When an object is updated all of the caches that have an edge with that object are invalidated along with the original object.
This approach is similar to using timestamps in that the objects whose timestamps are updated when another object is updated represent an edge in a dependency graph.

\subsubsection{Expiration times}
Another paper exploits the cache expiration time to invalidate caches.\cite{refreshingPerspectiveSearch}
The idea is that serving slightly stale data isn't terrible for search engines.
Additionally, the rate of change can be determined for a given query and can be factored into the expiration time.
This allows frequently changing data to have a shorter time to live and infrequently changing data to have a longer time to live.

\subsection{Determining what to cache}
Some research has been done looking into what data improves the effectiveness of the cache.
Many sources propose that fragments of HTML are the most useful data to cache.\cite{comparisonOfCachingSolutions}\cite{scalableConsistentCaching}
A cache hit for HTML allows the server to avoid generating the HTML as well as retrieving the underlying data from the persistent store.\cite{howBasecampGotSoFast}

One paper proposed that enforcing cache admission policies is a valuable way to improve hit ratio.\cite{cacheAdmissionPolicies}
The driving idea behind it is that there are many singleton queries (queries that are run only once) that pollute the cache because they are not used again.
Detecting such queries and leaving them out of the cache leaves more room for frequently accessed queries thus increasing cache hit ratio.
However, another paper claimed that the eviction policy doesn't have a significant impact on cache hit ratio.\cite{refreshingPerspectiveSearch}
This is because caches can easily be expanded to hold more entries and therefore items rarely get evicted to begin with.\cite{refreshingPerspectiveSearch}

Despite this claim, research has been done investigating eviction policies.
Least recently used (LRU) is a commonly used eviction policy.
There has been research into least likely to be used (LLU) which is an optimal policy that can be approximated by various techniques.


\section{Memcached}
Memcached.

\section{Metrics}
Metrics.

\chapter{Validation}
\label{validation}

\subsection{General Approach}
Validating my work is a fairly straightforward process.
The basic approach will involve gathering various statistics with and without my approach during an experiment based on a repeatable set of input data. %TODO: This sentence kinda sucks.

\subsection{Measured Values}
Some values that should be increased include cache hit rate and requests per second.
Some values that should be decreased include server load and page response time.
However, merely comparing the two sets of data will not be an accurate picture of the situation.
For example, cache hit rate may be increased as well as server load.
This could be the result of an optimistic prefetching algorithm that caches values before they are requested in the event that they will be requested soon.
The pivotal question is regarding whether or not the increased server load is worth the increased hit rate.

Other criteria, such as ease of use and cache freshness, are harder to measure and validate.
Cache freshness involves measuring how long a stale cache is served after there is a fresh version available.
In particular, some amount of cache staleness is acceptable but the line is blurred especially in the presence of performance gains.

Ease of use is even more difficult to measure.
A caching mechanism that requires a diligent programmer to carefully hand craft the caching algorithm with every feature added is not very easy to use.
On the other hand, a caching mechanism that operates behind the scenes without the programmer's knowledge is very easy to use.
Measuring this will be a very subjective matter.

\subsection{Experimental Setup}
Developing an experiment to get consistent results is crucial.
I am planning on recording all site traffic during a period of time and replaying it during the experiments.
This ensures that the workload will be the same across all tests.

First an unaltered log of site traffic will be replayed on both systems to emulate real world traffic patterns.
Next this log will be extrapolated to simulate increased site traffic.
Multiple levels of increased site traffic will be run on both systems as a comparison of their merits of scalability.
Scalability tests can be split into two separate groups: general traffic and targetted traffic.
General traffic isn't targetted to any specific pages; requests to all pages are uniformly increased.
This is similar to increasing a site's general popularity.
Targetted traffic on the other hand has increased traffic to a small subset of content.
This simulates special events such as press and social media where a page goes viral for a short amount of time.
Both types of traffic are realistic and are useful measures for the effectiveness of caching strategies.

The contents of the database and memcached must be the same at the start of the experiments.
The contents of memcached can be flushed to ensure a consistent repeatable experiment.
However, this setup isn't very realistic because the contents of cache is never completely empty in production and may skew results depending on differences in the caching algorithms.
To mitigate this issue, experiments can be run for a period of time before statistics are collected to make the contents of cache more realistic.

Ensuring the database is in a consistent state is a more difficult matter.
Simply performing a dump of the database won't work because of how long it takes to do so.
Instead, a slave database can be set up to be replicated from the master database.
The instant site traffic starts being recorded the slave will stop replication and the dump will be performed on the slave database.
The contents of the test database will be populated with this dump to guarantee a repeatable experiment.
The only problem with this approach is that populating a database from a MySQL dump may take a significant amount of tme depending on the size of the data.
This will merely increase the time between tests.

All experiments will be run on a duplicate set of machines that mirror the architecture that is used in production.
This ensures that the results gathered will be consistent with how they will perform in production.

\chapter{Conclusion}
\label{conclusion}
This paper has given a survey of caching methods used in web application code.
As you can see, different strategies work well for different types of data.
For example, write-through caching is very effective for simple data types.
More complex data, however, requires a caching strategy that can handle dependencies between objects.

There is still room for more research in this field.
In particular, an empirical study that determines the trade-offs between large and fine-grained caching would be a worthwhile contribution.


% ------------- End main chapters ----------------------

\clearpage
\bibliography{memcache-survey}
\bibliographystyle{plain}
%\addcontentsline{toc}{chapter}{Bibliography}

\end{document}
