
% Cal Poly Thesis
% 
% based on UC Thesis format
%
% modified by Mark Barry 2/07.
%


\documentclass[12pt]{ucthesis}

\usepackage{url}

\usepackage[pdftex]{graphicx}
% Update title and author below...
\usepackage[pdftex,plainpages=false,breaklinks=true,colorlinks=true,urlcolor=blue,citecolor=blue,%
                                   linkcolor=blue,bookmarks=true,bookmarksopen=true,%
                                   bookmarksopenlevel=3,pdfstartview=FitV,
                                   pdfauthor={Marc Zych},
                                   pdftitle={An Analysis of Generational Caching Implemented in a Production Website},
                                   pdfkeywords={thesis, masters, cal poly}
                                   ]{hyperref}
%Options with pdfstartview are FitV, FitB and FitH
\pdfcompresslevel=1

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[letterpaper]{geometry}
\usepackage[overload]{textcase}

\bibliographystyle{abbrv}

\setlength{\parindent}{0.25in} \setlength{\parskip}{6pt}

\geometry{verbose,nohead,tmargin=1.25in,bmargin=1in,lmargin=1.5in,rmargin=1.3in}

\setcounter{tocdepth}{2}


% Different font in captions (single-spaced, bold) ------------
\newcommand{\captionfonts}{\small\bf\ssp}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter
% ---------------------------------------




\begin{document}

% Declarations for Front Matter

\title{An Analysis of Generational Caching Implemented in a Production Website}
\author{Marc Zych}
\degreemonth{June} \degreeyear{2013} \degree{Master of Science}
\defensemonth{June} \defenseyear{2013}
\numberofmembers{3} \chair{Alexander Dekhtyar, Ph.D.} \othermemberA{Philip Nico, Ph.D.} \othermemberB{Chris Lupo, Ph.D.} \field{Computer Science} \campus{San Luis Obispo}
\copyrightyears{seven}



\maketitle

\begin{frontmatter}

% Custom made for Cal Poly (by Mark Barry, modified by Andrew Tsui).
\copyrightpage

% Custom made for Cal Poly (by Andrew Tsui).
\committeemembershippage

\begin{abstract}

Caching is hard. I'm going to make it better.

\end{abstract}


\tableofcontents


\listoftables

\listoffigures

\end{frontmatter}

\pagestyle{plain}




\renewcommand{\baselinestretch}{1.66}


\chapter{Introduction}
\label{introduction}

\section{Motivation}
Web scalability has been an issue for web sites since the dot-com boom of the late nineties. % TODO: Citation.
All moderately sized websites experience web server issues as their breadth of content as well as the number of active users grow.
If not properly addressed the website's performance suffers.
In some cases the site can get overloaded by requests and stop responding altogether.
Websites need to be able to scale with their site traffic to continue providing a good user experience.

Some parts of website scaling are very straightforward.
Serving static assets such as Cascading Style Sheets (CSS), JavaScript (JS), and images can be offloaded to Content Delivery Networks (CDN) that handle requests separately from other resources such as the page's HTML.
Scaling application machines can be accomplished by sending all incoming requests to a load balancer.
The load balancer then decides what application machine in the cluster of available machines should handle this request.
The request is forwarded to the selected machine which eventually returns the HTML of the page to the load balancer which then returns it to the user.
Although application machines can be scaled easily, they all still need to access the same underlying data which is typically a single database machine.
The database is the hardest resource to scale because of the inherent problems in distributing data while maintaining consistency.

% TODO: Split. Talk about company first then talk about website and scaling.
iFixit is a moderately sized website that is ``The free repair manual that you can edit.''
iFixit's step-by-step guides and question and answer platform are driven heavily by user-generated content.
All repair guides on iFixit are publicly editable and contain rich revision history.
This includes a patrolling system in which edits by unprivileged users are pending until a privileged user accepts them.
% TODO: Explain how "all kinds of folks" ask questions on Answers and members of the community answer and vote on them?
The consequence of these features is that all of the content is stored in a database.
All incoming requests for HTML need to be handled on demand based on content retrieved from the database.

\subsection{Caching}
The ubiquitous solution to scaling websites is some form of caching.
Although most web servers employ generous amounts of caching, the methods and approaches used vary widely.
Client side browser caching is popular because it avoids a large amount of network overhead.
This is typically done by determining whether or not the content has changed since it was last requested.
Proxy caching is very similar to browser caching except that it happens at proxy servers located in between the client and the primary host server.
This thesis, however, focuses on caching strategies and techniques in web application code that are aimed to reduce database load and decrease page response time.

% TODO: Diagrams?

\section{Main Contributions}
Main contributions.

The rest of this thesis is organized as follows...


\chapter{Background}
\label{background}

% TODO: Diagrams.

\section{More iFixit Background}
\subsection{Data and Site Usage}
iFixit's most popular content is their step-by-step repair guides.
An iFixit guide consists of meta data about the guide such as title, summary, introduction, author, time required, and difficulty.
Additionally, each guide has an ordered list of steps associated it.
Each step consists of a list of bullet points with text, indentation, and color as well some sort of media such as a list of images or a video.
To avoid duplicating, content guides can also be included in other guides as prerequisites.
The prerequisite guides' steps are included before the actual guide's steps.

All of iFixit's content is dynamic.
That is to say that handling requests involves gathering data from a database, rendering the page in the application language, and finally returning the resulting HTML to the user.
Additionally, anyone can edit the content due to it being a wiki.

iFixit certainly has a more read-heavy workload, however, writes are still fairly common.
In particular, iFixit is known for its device ``teardowns.''
These events consist of opening a brand new device as soon as possible.
The opening process is documented and updated on iFixit's website in realtime.
These events attract thousands of visitors to the site in a short period of time.
Because of this, the website must be able to handle a read-heavy workload while simultaneously writing to the same data.

In addition to their main website, iFixit runs a Software as a Service (SaaS) that provides a technical documentation platform to anyone who needs step-by-step instructions.
Much of this content is private so the general solutions to web scaling such as full page caching and web server proxy caching cannot be used.

The primary bottlneck is the database machine because it has to retrieve data for every incoming HTTP request.
On the contrary, application machines can easily scale horizontally by load balancing requests between them because they are stateless and operate independently of one another.

\subsection{Web Architecture}
% TODO: Diagrams.
In this section we describe iFixit's production architecture at the time of this writing.

iFixit runs two load balancers - one for iFixit and one for their SaaS.
At any given time there are typically four to eight application machines that run iFixit's core software which is written in PHP.
There is one master database machine running the Percona build of MySQL which handles all reads and writes.
There is also one slave database machine that is asynchronously replicated from the master database using MySQL's built in replication.
However, this machine is used purely as a backup and is not used to improve performance.

All of this is built using Amazon Web Services (AWS).
AWS services used include Elastic Cloud Compute (EC2) for server instances, S3 for storing static assets and images, CloudFront, a Content Delivery Network (CDN), for serving content out of S3, and Elastic Block Store (EBS) for raw block storage on each server instance.

\section{Caching}

\subsection{Terminology}
At a high level a cache is used to store results of operations so future requests can be processed faster.
A cache is typically not the primary storage location for data.
In most cases there is a permanent backing store such as a database which is used to retrieve information and populate the cache.
A cache hit occurs when the data that is being requested is in the cache and no further action is required.
A cache miss occurs when the data that is being requested is not in the cache and it must be retrieved from elsewhere.

Caching is effective because retrieving data from the cache is much faster than retrieving it from its persistent location.
Additionally, caching exploits locality.
Temporal locality is the idea that an item that has been referenced recently will likely be referenced again in the future.
Spatial locality is the idea that items in nearby storage locations are referenced in close proximity to one another.
As of late, spatial locality has been broaded for web sites to include access patterns such as recommended and related content.

Caches can be of the static or dynamic type.
A static cache is one that is not updated as a result of the application running.
The application can request data from the cache but it cannot add, modify, or remove cache entries.
However, the cache is periodically updated outside of the application to keep it mostly up to date.
The timeframe for this differs from application to application but is largely a function of how often the underlying data changes.
In contrast, a dynamic cache is one that can be updated as a result of the application execution.
The application can freely add, modify, and remove cache entries based on the operations it is performing due to requests.

Dynamic caches have a limited size and may eventually fill to capacity.
When this occurs items must be removed to make room for new ones.
The eviction policy determines which cache entries are removed.
Some caches allow entries to be set with an expire time.
After the specified time has passed the entry is expired and is no longer a valid cache entry and requests for it will miss.

Cache coherence, also referred to as cache consistency, is the concept of keeping the data in the cache consistent with the data stored in the primary backing store.
One such method is to perform cache invalidation upon data modification which removes any modified items from the cache so the data must be generated upon the next request.
A cache stampede can occur when a cache that is frequently requested is invalidated.
There are then a series of cache misses followed by a series of duplicated requests to the backing store followed by a series of cache sets.
These should be avoided if possible because of the redundant work done by each request.
If not handled properly, this event alone can crash web sites.


\section{iFixit's Current Caching Strategy}
iFixit currently has a very ad hoc approach to caching.
Any specific pages, database queries, or expensive operations are guarded by a cache get with a handcrafted cache key.
If the object is not in the cache then it is retrieved and stored in the cache for later use.
The expiration time is typically on the order of minutes to hours depending on the method of invalidation for the data in question.
iFixit uses two strategies for cache invalidation that are described below.

The first strategy relies on the cache's expiration time to update the entry with fresh data.
After the specified amount of time the cache is no longer valid so the application is forced to regenerate it and set it in the cache for later use.
The cache is never invalidated by the application when the underlying content changes.
This strategy is particularly useful for data that isn't required to be correct such as related content, approximate totals, etc.

The second strategy used by iFixit involves deleting all relevant cache entries whenever data is changed.
It is up to the developer to find the appropriate places to delete cache entries when certain events occur.
The advantages to this approach are that it's simple to understand, straightforward to implement, and if done correctly ensures that fresh data is served.

% TODO: Consider discussing varnish and page cache.

\section{Related Work}
\subsection{Research Challenges}
Most research is focused on addressing two core challenges with caching, the first of which is cache coherency.
In some cases maintaining cache coherency is trivial because there is only one cache associated with the underlying data.
However, with more complex data the situation is much more difficult.
There may be dependencies between objects which means that multiple caches need to be invalidated at the same time.
To further complicate things the dependencies between objects may be dynamic and must be resolved at run time on a per object basis.
This is very common for Web 2.0 sites that are driven by user-generated content.

Another research challenge is how to make the caching system easy to use for developers.
The problem is that manually controlling the cache is tedious and error-prone.
The idea is that this can be remedied by putting in place a system that transparently handles all caching operations which leaves the developer to write the more meaningful code.

\subsection{Incorporating Cache}
\subsubsection{Caching Technologies}
Most web application caching research revolves around caching strategies.
It turns out that the underlying cache daemon is largely irrelevant.
Memcached is by far the most popular web caching system; it is currently deployed by Facebook, Wikipedia, YouTube, and countless others \cite{memcachedDotOrg}.

The exception to this trend is a paper that proposes an alternative called SQLCached \cite{sqlCached}.
This tool offers a few things that memcached doesn't: complex data types and rich querying for retrieval and invalidation.
This is possible because SQLCached uses SQLite as its caching mechanism.

Memcached is an example of a dynamic cache daemon.
There are also static caches that are only updated periodically and are not modified directly by the web application.
Static caches are popular for search engines because a large number of search queries return the same data for long periods of time.
These queries can be determined by analyzing historical data which would be difficult to do using a dynamic cache \cite{designTradeOffsSearchEngine}.
In particular, the knapsack problem can be used to fill the static cache if the request frequency and size of cacheable items can be determined \cite{designTradeOffsSearchEngine}.
Additionally, many papers propose a 2 or 3-level cache that incorporates both static and dynamic caches \cite{cacheAdmissionPolicies}\cite{designTradeOffsSearchEngine}.

\subsubsection{Implementing in an Application}
Many approaches have been proposed for how to implement caching strategies in web application code.
The most straightforward method is to explicitly get, set, and invalidate caches wherever they are needed in the application.
However, this is tedious and error-prone \cite{keyBasedCacheExpiration}\cite{triggerBasedORM}.
A preferable alternative is to have caching built directly into the underlying application code.
For example, Ruby on Rails provides a very rich data model that handles most of the caching details behind the scenes \cite{keyBasedCacheExpiration}.

Another Object Relational Model (ORM) has been developed for the Django web framework that automatically handles all necessary caching operations \cite{triggerBasedORM}.
The project, CacheGenie, provides Python callbacks to database triggers that will be called when queries are run on the database. This allows the tool to update and/or invalidate caches depending on the queries.
The major advantage of this approach is that no matter how the data is changed the cache remains consistent because cache invalidation occurs at the data layer.
Other projects have used database triggers with similar results \cite{scalableConsistentCaching}.

\subsection{Cache Consistency}
\subsubsection{Write-Through Caching}
Write-through caching is a technique that involves saving the data in the cache at the same as it is being saved in the persistent store \cite{writeThroughCaching}.
This technique improves cache hit rate because cache misses will only happen as a result of cached values being evicted or a server failure.
However, this technique only works for simple data types.
Any other objects that depend on the current object being saved can't be updated in the cache because that data isn't directly accessible and will require a database lookup.

\subsubsection{Timestamps}
Bai and Junqueira proposed a system that does not invalidate caches but rather determines cache freshness upon cache hit \cite{cacheInvalidationWebSearch}.
This is done by including a timestamp corresponding to the generation time in the cached object.
Upon cache hit the timestamp is compared against a query index that is updated as the underlying data is updated.

Another caching technique is referred to as key based cache expiration \cite{keyBasedCacheExpiration} or generational caching \cite{generationalCaching}.
The idea is that a timestamp corresponding to the last time the object was updated is included in the key for any caches that depend on it.
The timestamp is updated once the object is modified thus invalidating all necessary cache keys.
This approach also updates the timestamps of any objects that depend on the modified object.
This technique is employed by 37Signals and various other websites \cite{keyBasedCacheExpiration}.

\subsubsection{Prefetching}
Prefetching content is another common solution for caching.
This technique involves caching content before it is requested so it can be retrieved quickly when it is needed.
Generally an event triggers the precaching system to intelligently decide what content to prefetch in the hopes that the data will be requested soon.
This approach takes advantage of spatial locality.

A caching system was developed that addresses the problem of cache stampedes \cite{scalableConsistentCaching}.
Rather than invalidating any necessary caches, the content can be prefetched and the cache's value will be updated in place using write-through caching.
This approach avoids cache misses at the cost of slight data staleness because the old copy will still be served while the fresh copy is being prefetched.
The website for the 1998 Winter Olympic games deployed this caching technique and reached nearly 100\% cache hits \cite{scalableConsistentCaching}.

Challenger, Iyengar, and Dantzig describe a system that constructs a dependency graph that is used when invalidating caches \cite{scalableConsistentCaching}.
When an object is updated all of the caches that have an edge with that object are invalidated along with the original object.
This approach is similar to using timestamps in that the objects whose timestamps are updated when another object is updated represent an edge in a dependency graph.

Another paper exploits the cache expiration time to invalidate caches \cite{refreshingPerspectiveSearch}.
The idea is that serving slightly stale data isn't terrible for search engines.
Additionally, the rate of change can be determined for a given query and can be factored into the expiration time.
This allows frequently changing data to have a shorter time to live and infrequently changing data to have a longer time to live.

\subsection{Determining what to cache}
Some research has been done looking into what data improves the effectiveness of the cache.
Many sources propose that fragments of HTML are the most useful data to cache \cite{comparisonOfCachingSolutions}\cite{scalableConsistentCaching}.
A cache hit for HTML allows the server to avoid generating the HTML as well as retrieving the underlying data from the persistent store \cite{howBasecampGotSoFast}.

Baeza-Yates et al. proposed that enforcing cache admission policies is a valuable way to improve hit ratio \cite{cacheAdmissionPolicies}.
The driving idea behind it is that there are many singleton queries (queries that are run only once) that pollute the cache because they are not used again.
Detecting such queries and leaving them out of the cache leaves more room for frequently accessed queries thus increasing cache hit ratio.
However, another paper claimed that the eviction policy doesn't have a significant impact on cache hit ratio \cite{refreshingPerspectiveSearch}.
This is because caches can easily be expanded to hold more entries and therefore items rarely get evicted to begin with \cite{refreshingPerspectiveSearch}.

Despite this claim, research has been done investigating eviction policies.
Least recently used (LRU) is a commonly used eviction policy.
There has been research into least likely to be used (LLU) which is an optimal policy that can be approximated by various techniques.


\section{Memcached}
``Memcached is an in-memory key-value store for small chunks of arbitrary data (strings, objects) from results of database calls, API calls, or page rendering.''\cite{memcachedDotOrg}
There are two main operations in Memcached: {\tt get} and {\tt set}.
Clients perform {\tt get} calls by passing in a string cache key that identifies the data that is being requested.
If the data is in the cache it is returned to the caller, otherwise the result is empty which indicates that the application must retrieve the data elsewhere.
Data is added to Memcached by calling {\tt set} with a cache key, expiration time, and the data being stored in the cache.
Another specialty command is {\tt getMulti} which is used to retrieve multiple keys in one operation.
This is used to improve client performance by reducing the network overhead associated with retrieving many keys in succession.
All Memcached operations are constant time ({\tt O(1)}) with respect to the number of elements stored in it.

There are three ways for cached data to be removed from Memcached.
The expire time the data was set with determines the maximum lifespan of the cache for which it is valid.
Once the data has expired it is no longer returned for {\tt get} requests although the data is not immediately removed from memory.
Before data has expired, however, items can be evicted due to memory pressure.
This occurs when new items are being added to the cache and there isn't any room.
Expired items are evicted first if any are found.
If not the LRU policy is used to select a key to evict.
Finally, cache entries can also be removed by performing {\tt delete} requests and providing the key that identifies the data to be deleted.

To reduce the effects of memory fragmentation, Memcached employs a technique called slab allocation.
The total memory available to Memcached is partitioned into 1MB pages.
Pages are assigned to slab-classes on an as-needed basis.
Slab-classes determine the maximum size of data to be stored in the slab and consequently the size of chunks in pages assigned to the slab-class.
When storing data in Memcached, the slab-class with the smallest chunk size that the data can fit in is used.
Any remaining space in the chunk cannot be used to store any other data and therefore ends up being wasted space.
However, the amount of space wasted in slab allocation is less than a naive implementation which results in memory fragmentation. % TODO: naive needs oomlaut(sp?)?
All replacement algorithms operate on a per-slab-class basis.

Memcached is distributed in nature.
Adding more servers to a web application involves adding the new Memcached instance's IP address and port to the list of servers that the Memcached client library uses.
Keys map to exactly one server so the total memory available in the Memcached cluster is the sum of memory available on each instance
The client library performs consistent hashing on any key lookups to determine which server in the cluster to send the request to.
The result is that adding and removing servers doesn't drastically impact hit rate because the majority of keys map to the same servers before and after the change.

\section{Metrics}
This data is more or less in the Measured Values section of Validation.
We could either migrate that section here or remove this section.

\chapter{Validation}
\label{validation}
This section was written for 590 when I didn't have a clear topic so this will have to be tweaked once I actually run the experiments.
Other similar papers should be consulted to get a good experimental setup that is accepted in the community.

\section{General Approach}
Validating my work is a fairly straightforward process.
The basic approach will involve gathering various statistics with and without my approach during an experiment based on a repeatable set of input data. %TODO: This sentence kinda sucks.

\section{Measured Values}
Some values that should be increased include cache hit rate and requests per second.
Some values that should be decreased include server load and page response time.
However, merely comparing the two sets of data will not be an accurate picture of the situation.
For example, cache hit rate may be increased as well as server load.
This could be the result of an optimistic prefetching algorithm that caches values before they are requested in the event that they will be requested soon.
The pivotal question is regarding whether or not the increased server load is worth the increased hit rate.

Other criteria, such as ease of use and cache freshness, are harder to measure and validate.
Cache freshness involves measuring how long a stale cache is served after there is a fresh version available.
In particular, some amount of cache staleness is acceptable but the line is blurred especially in the presence of performance gains.

Ease of use is even more difficult to measure.
A caching mechanism that requires a diligent programmer to carefully hand craft the caching algorithm with every feature added is not very easy to use.
On the other hand, a caching mechanism that operates behind the scenes without the programmer's knowledge is very easy to use.
Measuring this will be a very subjective matter.

\section{Experimental Setup}
Developing an experiment to get consistent results is crucial.
I am planning on recording all site traffic during a period of time and replaying it during the experiments.
This ensures that the workload will be the same across all tests.

First an unaltered log of site traffic will be replayed on both systems to emulate real world traffic patterns.
Next this log will be extrapolated to simulate increased site traffic.
Multiple levels of increased site traffic will be run on both systems as a comparison of their merits of scalability.
Scalability tests can be split into two separate groups: general traffic and targetted traffic.
General traffic isn't targetted to any specific pages; requests to all pages are uniformly increased.
This is similar to increasing a site's general popularity.
Targetted traffic on the other hand has increased traffic to a small subset of content.
This simulates special events such as press and social media where a page goes viral for a short amount of time.
Both types of traffic are realistic and are useful measures for the effectiveness of caching strategies.

The contents of the database and memcached must be the same at the start of the experiments.
The contents of memcached can be flushed to ensure a consistent repeatable experiment.
However, this setup isn't very realistic because the contents of cache is never completely empty in production and may skew results depending on differences in the caching algorithms.
To mitigate this issue, experiments can be run for a period of time before statistics are collected to make the contents of cache more realistic.

Ensuring the database is in a consistent state is a more difficult matter.
Simply performing a dump of the database won't work because of how long it takes to do so.
Instead, a slave database can be set up to be replicated from the master database.
The instant site traffic starts being recorded the slave will stop replication and the dump will be performed on the slave database.
The contents of the test database will be populated with this dump to guarantee a repeatable experiment.
The only problem with this approach is that populating a database from a MySQL dump may take a significant amount of tme depending on the size of the data.
This will merely increase the time between tests.

All experiments will be run on a duplicate set of machines that mirror the architecture that is used in production.
This ensures that the results gathered will be consistent with how they will perform in production.


% ------------- End main chapters ----------------------

\clearpage
\bibliography{thesis}
\bibliographystyle{plain}
%\addcontentsline{toc}{chapter}{Bibliography}

\end{document}
