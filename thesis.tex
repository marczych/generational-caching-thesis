
% Cal Poly Thesis
% 
% based on UC Thesis format
%
% modified by Mark Barry 2/07.
%


\documentclass[12pt]{ucthesis}

\usepackage{url}

\usepackage[pdftex]{graphicx}
% Update title and author below...
\usepackage[pdftex,plainpages=false,breaklinks=true,colorlinks=true,urlcolor=blue,citecolor=blue,%
                                   linkcolor=blue,bookmarks=true,bookmarksopen=true,%
                                   bookmarksopenlevel=3,pdfstartview=FitV,
                                   pdfauthor={Marc Zych},
                                   pdftitle={An Analysis of Generational Caching Implemented in a Production Website},
                                   pdfkeywords={thesis, masters, cal poly}
                                   ]{hyperref}
%Options with pdfstartview are FitV, FitB and FitH
\pdfcompresslevel=1

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[letterpaper]{geometry}
\usepackage[overload]{textcase}
\usepackage{minted}

\bibliographystyle{abbrv}

\setlength{\parindent}{0.25in} \setlength{\parskip}{6pt}

\geometry{verbose,nohead,tmargin=1.25in,bmargin=1in,lmargin=1.5in,rmargin=1.3in}

\setcounter{tocdepth}{2}


% Different font in captions (single-spaced, bold) ------------
\newcommand{\captionfonts}{\small\bf\ssp}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter
% ---------------------------------------




\begin{document}

% Declarations for Front Matter

\title{An Analysis of Generational Caching Implemented in a Production Website}
\author{Marc Zych}
\degreemonth{June} \degreeyear{2013} \degree{Master of Science}
\defensemonth{June} \defenseyear{2013}
\numberofmembers{3} \chair{Alexander Dekhtyar, Ph.D.} \othermemberA{Philip Nico, Ph.D.} \othermemberB{Chris Lupo, Ph.D.} \field{Computer Science} \campus{San Luis Obispo}
\copyrightyears{seven}



\maketitle

\begin{frontmatter}

% Custom made for Cal Poly (by Mark Barry, modified by Andrew Tsui).
\copyrightpage

% Custom made for Cal Poly (by Andrew Tsui).
\committeemembershippage

\begin{abstract}

Caching is hard. I'm going to make it better.

\end{abstract}


\tableofcontents


\listoftables

\listoffigures

\end{frontmatter}

\pagestyle{plain}




\renewcommand{\baselinestretch}{1.66}


\chapter{Introduction}
\label{introduction}

\section{Motivation}
Web scalability has been an issue for web sites since the dot-com boom of the late nineties \cite{webServerScaling}.
All moderately sized websites experience web server issues as their breadth of content as well as the number of active users grow.
If not properly addressed, the website's performance suffers.
In some cases the site can get overloaded by requests and stop responding altogether.
Websites need to be able to scale with their site traffic to continue providing a good user experience.

Some parts of website scaling are very straightforward.
Serving static assets such as Cascading Style Sheets (CSS), JavaScript (JS), and images can be offloaded to Content Delivery Networks (CDN) that handle requests separately from other resources such as the page's HTML.
Scaling application machines can be accomplished by sending all incoming requests to a load balancer.
The load balancer then decides what application machine in the cluster of available machines should handle this request.
The request is forwarded to the selected machine which eventually returns the HTML of the page to the load balancer which then returns it to the user.
Although application machines can be scaled easily, they all still need to access the same underlying data which is typically a single database machine.
The database is the hardest resource to scale because of the inherent problems in distributing data while maintaining consistency.

The ubiquitous solution to scaling websites is some form of caching.
Although most web servers employ generous amounts of caching, the methods and approaches used vary widely.
Client side browser caching is popular because it avoids a large amount of network overhead.
This is typically done by determining whether or not the content has changed since it was last requested.
Proxy caching is very similar to browser caching except that it happens at proxy servers located in between the client and the primary host server.
This thesis, however, focuses on caching strategies and techniques in web application code that are aimed to reduce database load and decrease page response time.

Although caching is used widely in industry, it is not a solved problem.
Websites still struggle with determining what data to cache and how fine-grained to make the caches.
Additionally, cache invalidation is the most important aspect of caching strategies and commonly is the driving force behind new solutions.
This is primarily because websites are becoming less tolerant of serving stale data especially when their content is constantly changing.
Current research is focused on maintaining a high cache hit rate while maintaining cache coherency.

Modern websites are interested in caching as a way to handle increasing volumes of traffic.
Traditional website scaling techniques aren't effective because of the vast amounts of user-generated content and dynamic nature of websites.
Nearly all web pages need to be generated on demand from content out of the database.
Because of this, server load becomes a real issue especially for the database machine.

One such website facing server scaling problems is {\textsf iFixit}.
{\textsf iFixit} is a moderately sized website that is ``the free repair manual that you can edit''\cite{ifixitDotCom}.
{\textsf iFixit}'s step-by-step guides and question and answer platform are driven heavily by user-generated content.
All repair guides on iFixit are publicly editable and contain rich revision history.
This includes a patrolling system in which edits by unprivileged users are pending until a privileged user accepts them.
The consequence of these features is that all of the content is stored in a database.
{\textsf iFixit} currently employs an ad hoc caching strategy that relieves some server load, however, the effectiveness of the cache can be greatly improved.

% TODO: Diagrams?

\section{Main Contributions}
Main contributions.

The rest of this thesis is organized as follows...


\chapter{Background}
\label{background}

% TODO: Diagrams.

\section{iFixit}
\subsection{Data and Site Usage}
iFixit's most popular content is their step-by-step repair guides.
An iFixit guide consists of meta data about the guide such as title, summary, introduction, author, time required, and difficulty.
Additionally, each guide has an ordered list of steps associated it.
Each step consists of a list of bullet points with text, indentation, and color as well some sort of media such as a list of images or a video.
To avoid duplicating content, guides can also be included in other guides as prerequisites.
The prerequisite guides' steps are included before the actual guide's steps.

All of iFixit's content is dynamic.
That is to say that handling requests involves gathering data from a database, rendering the page in the application language, and finally returning the resulting HTML to the user.
Additionally, anyone can edit the content due to it being a wiki.

iFixit certainly has a more read-heavy workload, however, writes are still fairly common.
In particular, iFixit is known for its device ``teardowns.''
These events consist of opening a brand new device as soon as it becomes available on the market.
The opening process is documented and updated on iFixit's website in realtime.
These events attract thousands of visitors to the site in a short period of time.
Because of this, the website must be able to handle a read-heavy workload while simultaneously writing to the same data.

In addition to their main website, iFixit runs a Software as a Service (SaaS) platform for technical documentation platform to anyone who needs step-by-step instructions.
The general solutions to web scaling, such as full page caching and web server proxy caching, cannot be used because much of this content is private.

The primary bottlneck is the database machine because it has to retrieve data for every incoming HTTP request.
On the contrary, application machines can easily scale horizontally by load balancing requests between them because they are stateless and operate independently of one another.

\subsection{Web Architecture}
% TODO: Diagrams.
In this section we describe iFixit's production architecture at the time of this writing.

iFixit runs two load balancers - one for iFixit and one for their SaaS.
At any given time there are typically four to eight application machines that run iFixit's core software which is written in PHP.
There is one master database machine running the Percona build of MySQL which handles all reads and writes.
There is also one slave database machine that is asynchronously replicated from the master database using MySQL's built in replication.
However, this machine is used purely as a backup and is not used to improve performance.

All of this is built using Amazon Web Services (AWS).
AWS services used include Elastic Cloud Compute (EC2) for server instances, S3 for storing static assets and images, CloudFront, a Content Delivery Network (CDN), for serving content out of S3, and Elastic Block Store (EBS) for raw block storage on each server instance.

\section{Terminology}
At a high level, a cache is used to store results of operations so future requests can be processed faster.
A cache is typically not the primary storage location for data.
In most cases there is a permanent backing store such as a database which is used to retrieve information and populate the cache.
A cache hit occurs when the data that is being requested is in the cache and no further action is required.
A cache miss occurs when the data that is being requested is not in the cache and it must be retrieved from elsewhere.

Caching is effective because retrieving data from the cache is much faster than retrieving it from its persistent location.
Additionally, caching exploits locality.
Temporal locality is the idea that an item that has been referenced recently will likely be referenced again in the future.
Spatial locality is the idea that items in nearby storage locations are referenced in close proximity to one another.
As of late, spatial locality has been broadened for web sites to include access patterns such as recommended and related content.

Caches can be of the static or dynamic type.
A static cache is one that is not updated as a result of the application running.
The application can request data from the cache but it cannot add, modify, or remove cache entries.
However, the cache is periodically updated outside of the application to keep it mostly up to date.
The timeframe for this differs from application to application but is largely a function of how often the underlying data changes.
In contrast, a dynamic cache is one that can be updated as a result of the application execution.
The application can freely add, modify, and remove cache entries based on the operations it is performing due to requests.

Dynamic caches have a limited size and may eventually fill to capacity.
When this occurs items must be removed to make room for new ones.
The eviction policy determines which cache entries are removed.
Some caches allow entries to be set with an expire time.
After the specified time has passed the entry is expired and is no longer a valid cache entry and requests for it will miss.

Cache coherence, also referred to as cache consistency, is the concept of keeping the data in the cache consistent with the data stored in the primary backing store.
One such method is to perform cache invalidation upon data modification which removes any modified items from the cache so the data must be generated upon the next request.
A cache stampede can occur when a cache that is frequently requested is invalidated.
There are then a series of cache misses followed by a series of duplicated requests to the backing store followed by a series of cache sets.
These should be avoided if possible because of the redundant work done by each request.
If not handled properly, this event alone can crash web sites.

% TODO: This might not really belong here.
The ActiveRecord pattern is a common technique used in web applications to interact with the data stored in a database.
In the host language, the developer creates classes that represent tables in the database.
This becomes an object-relational mapping (ORM) in that the properties of the class represent columns in the database.
Rows can be inserted, updated, and removed by calling the {\tt insert}, {\tt update}, {\tt delete} methods on the object, respectively.
This is demonstrated in Figure ~\ref{fig:activeRecordExample}.

% TODO: Make code single spaced.
\begin{figure}[h]
\begin{minted}{php}
<?php
function createUser($userid, $name, $email) {
   $user = new User();
   $user->userid = $userid;
   $user->name = $name;
   $user->email = $email;
   $user->insert();
}

function getUser($userid) {
   $user = User::find($userid);
}
?>
\end{minted}
\caption{Basic ActiveRecord example.}
\label{fig:activeRecordExample}
\end{figure}

In this example, {\tt User} is a class that implements the ActiveRecord pattern.
A new user is inserted into the database by creating a new {\tt User} object, assigning the properties of the object corresponding to the database columns, and calling {\tt insert}.
This {\tt User} can later be retrieved by calling {\tt find}.


\section{iFixit's Current Caching Strategy}
iFixit currently has a very ad hoc approach to caching.
Any specific pages, database queries, or expensive operations are guarded by a cache {\tt get} with a handcrafted cache key.
If the object is not in the cache then it is retrieved and stored in the cache for later use.
The expiration time is typically on the order of minutes to hours depending on the method of invalidation for the data in question.
iFixit uses two strategies for cache invalidation that are described below.

The first strategy relies on the cache's expiration time to update the entry with fresh data.
After the specified amount of time, the cache is no longer valid so the application is forced to regenerate it and set it in the cache for later use.
The cache is never invalidated by the application when the underlying content changes.
This strategy is particularly useful for data that isn't required to be correct such as related content, approximate totals, etc.

The second strategy used by iFixit involves deleting cache entries whenever relevant data is changed.
It is up to the developer to find the appropriate places to delete cache entries when certain events occur.
The advantages to this approach are that it's simple to understand, straightforward to implement, and ensures that fresh data is served if done correctly.

There are two major problems with these techniques.
The first of which is that the burden of cache invalidation is on the developer.
This is because the developer must decide what data to cache and spend time finding all the places where the caches should be invalidated.
Additionally, this is error-prone because software projects are constantly changing and the fragile caching setup could break.

The next problem is that caching HTML is difficult because each cache entry must be invalidated when data changes.
The more cache entries that depend on data the harder it is to ensure that they will all be invalidated at the appropriate time.
This is especially true for fragments of HTML because the cache entries are typically defined in front-end templates but must be invalidated from backend code.
In general, the difficulty of cache invalidation for any given data is proportional to the number of cache entries dependent on that data and how fine-grained those cache entries are.
As a result, iFixit doesn't currently cache any HTMl fragments.

% TODO: Discuss weaknesses. Justify needs for your work! (Mainly done but might want to specify why caching is still needed.)

% TODO: Consider discussing varnish and page cache.

\section{Related Work}
\subsection{Research Challenges}
Most research is focused on addressing two core challenges with caching: cache coherency and ease-of-use.
In some cases maintaining cache coherency is trivial because there is only one cache associated with the underlying data.
However, with more complex data the situation is much more difficult.
There may be dependencies between objects which means that multiple caches need to be invalidated at the same time.
To further complicate things the dependencies between objects may be dynamic and must be resolved at run time on a per object basis.
This is very common for Web 2.0 sites that are driven by user-generated content.

The second core caching challenge is ease-of-use.
The problem is that manually controlling the cache is tedious and error-prone.
Leaving it up to the application developer to correctly {\tt get}, {\tt set}, and {\tt delete} caches results in an ad-hoc caching strategy.
Most research regarding ease-of-use focuses on developing a system that transparently handles all caching operations.
This leaves the developer to write application-specific code instead of caching boilerplate.

\subsection{Incorporating Cache}
\subsubsection{Caching Technologies}
Most web application caching research revolves around caching strategies.
It turns out that the underlying cache daemon is largely irrelevant.
{\textsf Memcached} is by far the most popular web caching system; it is currently deployed by Facebook, Wikipedia, YouTube, and countless others \cite{memcachedDotOrg}.

{\textsf Memcached} is an example of a dynamic cache daemon.
There are also static caches that are only updated periodically and are not modified directly by the web application.
Static caches are popular for search engines because a large number of search queries return the same data for long periods of time.
These queries can be determined by analyzing historical data which would be difficult to do using a dynamic cache \cite{designTradeOffsSearchEngine}.
In particular, the knapsack problem can be used to fill the static cache if the request frequency and size of cacheable items can be determined \cite{designTradeOffsSearchEngine}.
Additionally, many papers propose a 2 or 3-level cache that incorporates both static and dynamic caches \cite{cacheAdmissionPolicies, designTradeOffsSearchEngine}.

The exception to this trend is Voras and Zagar's {\textsf Memcached} alternative named SQLCached \cite{sqlCached}.
This tool offers a few things that {\textsf Memcached} doesn't: complex data types and rich querying for retrieval and invalidation.
This is possible because SQLCached uses SQLite as its caching mechanism.

\subsubsection{Implementing in an Application}
Many approaches have been proposed for how to implement caching strategies in web application code.
The most straightforward method is to explicitly get, set, and invalidate caches wherever they are needed in the application.
However, this is tedious and error-prone \cite{keyBasedCacheExpiration, triggerBasedORM}.
A preferable alternative is to have caching built directly into the underlying application code.
For example, Ruby on Rails, a popular web framework, provides a very rich data model that handles most of the caching details behind the scenes \cite{keyBasedCacheExpiration}.

Gupta, Zeldovich, and Madden developed Cache Genie, an ORM for the Django web framework that automatically handles all necessary caching operations \cite{triggerBasedORM}.
The project provides Python callbacks to database triggers that are called when queries are run on the database.
This allows the tool to update and/or invalidate caches depending on the queries.
The major advantage of this approach is that no matter how the data is changed the cache remains consistent because cache invalidation occurs at the data layer.
Other projects have used database triggers with similar results \cite{scalableConsistentCaching}.

\subsection{Cache Consistency}
\subsubsection{Write-Through Caching}
Write-through caching is a technique that involves saving the data in the cache at the same time as it is being saved in the persistent store \cite{writeThroughCaching}.
This technique improves cache hit rate because cache misses will only happen as a result of cached values being evicted or a server failure.
However, this technique only works for simple data types.
Any other objects that depend on the current object being saved can't be updated in the cache because that data isn't directly accessible and will require a database lookup.

\subsubsection{Generational Caching}
Another caching technique is referred to as key-based cache expiration \cite{keyBasedCacheExpiration} and generational caching \cite{generationalCaching}.
The idea is that a timestamp, which corresponds to the last time the object was updated, is included in the key for any caches that depend on it.
The timestamp is updated once the object is modified thus invalidating all necessary cache keys.
This approach also updates the timestamps of any objects that depend on the modified object.
Generational caching makes fine-grained caching trivial because it isn't necessary to enumerate all potential cache keys.
Cache entries can be defined arbitrarily and as long as the cache key contains the timestamp it will be invalidated when the underlying object is updated.
This technique is employed by 37Signals and various other websites \cite{keyBasedCacheExpiration}.

\subsubsection{Prefetching}
Prefetching content is another common solution for caching.
This technique involves caching content before it is requested so it can be retrieved quickly when it is needed.
Generally, an event triggers the precaching system to intelligently decide what content to prefetch in the hopes that the data will be requested soon.
This approach takes advantage of spatial locality.

Challenger, Iyengar, and Dantzig developed a caching system that addresses the problem of cache stampedes \cite{scalableConsistentCaching}.
Rather than invalidating any necessary caches when data changes, the fresh content can be prefetched and the cache's value will be updated in place using write-through caching.
This approach avoids cache misses at the cost of slight data staleness because the old copy will still be served while the fresh copy is being prefetched.
The website for the 1998 Winter Olympic games deployed this caching technique and reached nearly 100\% cache hits \cite{scalableConsistentCaching}.
Their system uses a dependency graph to determine which cache entries need to be updated when any given data is modified  \cite{scalableConsistentCaching}.
When an object is updated, all of the caches that have an edge with that object need to be prefetched along with the original object.

Cambazoglu, Junqueira, and Plachouras use the cache expiration time to invalidate caches \cite{refreshingPerspectiveSearch}.
The idea is that serving slightly stale data isn't terrible for search engines.
Additionally, the rate of change can be determined for a given query and can be factored into the expiration time.
This allows frequently changing data to have a shorter time to live and infrequently changing data to have a longer time to live.

\subsection{Determining what to cache}
Some research has been done trying to determine what data is the most useful to cache.
Many sources propose that fragments of HTML are the most useful \cite{comparisonOfCachingSolutions, scalableConsistentCaching}.
A cache hit for HTML allows the server to avoid generating the HTML as well as retrieving the underlying data from the persistent store \cite{howBasecampGotSoFast}.
% TODO: Expand on this since it's an important part of my system. Mention pitfalls here or elsewhere?

Baeza-Yates et al. suggested that enforcing cache admission policies is a valuable way to improve hit ratio \cite{cacheAdmissionPolicies}.
The driving idea behind it is that there are many singleton queries (queries that are run only once) that pollute the cache because they are not used again.
Detecting such queries and leaving them out of the cache leaves more room for caching results of frequently accessed queries thus increasing cache hit ratio.
However, Cambazoglu, Junqueira, and Plachouras claim that the eviction policy doesn't have a significant impact on cache hit ratio \cite{refreshingPerspectiveSearch}.
This is because caches can easily be expanded to hold more entries and therefore items rarely get evicted to begin with \cite{refreshingPerspectiveSearch}.

% TODO: Delete?
Despite this claim, research has been done investigating eviction policies.
Least recently used (LRU) is a commonly used eviction policy.
There has been research into least likely to be used (LLU) which is an optimal policy that can be approximated by various techniques.


\section{Memcached}
% TODO: Explain LRU.
In the words of its creators, \begin{quotation}``Memcached is an in-memory key-value store for small chunks of arbitrary data (strings, objects) from results of database calls, API calls, or page rendering.\cite{memcachedDotOrg}''\end{quotation}
There are two main operations in {\textsf Memcached}: {\tt get} and {\tt set}.
Clients perform {\tt get} calls by passing in a string cache key that identifies the data that is being requested.
If the data is in the cache it is returned to the caller, otherwise the result is empty which indicates that the application must retrieve the data elsewhere.
Data is added to {\textsf Memcached} by calling {\tt set} with a cache key, expiration time, and the data being stored in the cache.
Another specialty command is {\tt getMulti} which is used to retrieve multiple keys in one operation.
This is used to improve client performance by reducing the network overhead associated with retrieving many keys in succession.
All {\textsf Memcached} operations are constant time ({\tt O(1)}) with respect to the number of elements stored in it.

There are three ways for cached data to be removed from {\textsf Memcached}.
The expire time the data was set with determines the maximum lifespan of the cache for which it is valid.
Once the data has expired it is no longer returned for {\tt get} requests although the data is not immediately removed from memory.
Before data has expired, however, items can be evicted due to memory pressure.
This occurs when new items are being added to the cache and there isn't any room.
Expired items are evicted first if any are found.
If not the LRU policy is used to select a key to evict.
Finally, cache entries can also be removed by performing {\tt delete} requests and providing the key that identifies the data to be deleted.

To reduce the effects of memory fragmentation, {\textsf Memcached} employs a technique called slab allocation.
The total memory available to {\textsf Memcached} is partitioned into 1MB pages.
Pages are assigned to slab-classes on an as-needed basis.
Slab-classes determine the maximum size of data to be stored in the slab and consequently the size of chunks in pages assigned to the slab-class.
When storing data in {\textsf Memcached}, the slab-class with the smallest chunk size that the data can fit in is used.
Any remaining space in the chunk cannot be used to store any other data and therefore ends up being wasted space.
However, the amount of space wasted in slab allocation is less than a naive implementation which results in memory fragmentation.
All replacement algorithms operate on a per-slab-class basis.

{\textsf Memcached} is distributed in nature.
Adding more servers to a web application involves adding the new {\textsf Memcached} instance's IP address and port to the list of servers that the {\textsf Memcached} client library uses.
Keys map to exactly one server so the total memory available in the {\textsf Memcached} cluster is the sum of memory available on each instance
The client library performs consistent hashing on any key lookups to determine which server in the cluster to send the request to.
The result is that adding and removing servers doesn't drastically impact hit rate because the majority of keys map to the same servers before and after the change.


\chapter{Design and Implementation}
\label{designAndImplementation}
In this chapter, we explain in depth the caching solution we analyze in Chapter X.
In section 3.1 we explain the high-level design of the generational caching strategy implemented for iFixit.
In section 3.2 we go over the details of implementing the design in iFixit's code base.

% TODO: Reiterate Requirements.

\section{Design}
\subsection{What is being cached}
This section describes the three primary types of data that are cached.
The first data type are objects constructed from query results that represent a tuple in the database.
These are used as an ORM and can be used to make modifications to the database as well as just retrieve data for consumption.
The second data type are fragments of HTML.
These are typically generated from data retrieved from the database and need to be invalidated anytime the underlying data has changed.
Finally, results of ad hoc queries are also cached.
These typically are used in addition to the ORM to gather more complex data.
%TODO: Picture?

\subsection{ORM Caching}
The primary key uniquely identifies a tuple in the database and can be used with the ORM to select a single record.
Once the record is loaded, other data not contained in the primary table is loaded and then the entire object is cached.
By default, the cache key for objects is just the primary key.

Generational caching relies on a value in the primary table being updated whenever any other data in the table is updated.
This value can be incorporated into the object's cache key which ensures that the value in the cache will always be correct for that cache key.
If the data changes then it has a different cache key so the other caches don't need to be invalidated.
To accomplish this, individual classes extending the ORM specify what columns in addition to the primary key to include in the cache key.
In most cases the additional columns are not known before retrieving the object.
In this case, they must be retrieved before checking the cache.
To avoid hitting the database as much as possible, the entire database tuple that is used to generate the cache key is cached using its primary key.
This cache is deleted anytime the object is saved.

With the entire database tuple we can construct the full cache key and check for its presence in the cache.
If it exists in the cache then it can be returned to the caller immediately.
If it doesn't then it must be retrieved from the database.
Fortunately the primary table tuple has already been fetched from the database so we only need to load the additional data for the object and store it in the cache.

This strategy works especially well when retrieving an object's additional data is very expensive.
If no additional data is loaded then the ``fully loaded'' object is the exact same as the one fetched when gathering more data to construct the cache key.
In this case there is no benefit to using generational caching because the data cached by the primary key is the same as the data cached with the full set of columns.
However, the additional columns can be used in cache keys for data that depends on it to avoid cache invalidation.
The amount of extra data loaded in addition to the primary table to make the overhead of generational caching worthwhile varies on a case-by-case basis.

\section{Implementation}
\subsection{Cache-Aware ORM}
Most of the work done involved augmenting iFixit's ORM to cache objects when they are retrieved.
The primary operation for retrieving data through the ORM is {\tt findAllByKey} which creates objects based on their primary keys in the database.
A list of primary keys is provided and the ORM selects those tuples from the database and returns the objects constructed from them.

In order to add caching to {\tt findAllByKey} we add a wrapper method called {\tt findAllCached}.
At a very high-level, this method constructs cache keys for the given primary keys and performs a cache get on them.
Any objects that were not cached are retrieved from the database using {\tt findAllByKey} and are then stored in the cache.
% TDOO: Picture.

We allow classes derived from the ORM to specify which columns in addition to the primary key to include in the cache key.
More data must be retrieved from the database to construct the cache key if only the primary keys are provided to {\tt findAllCached} and the derived class specifies additional cache key columns.
Fortunately we already have a method, {\tt findAllBykey}, that retrieves objects based on their primary key that can be used to retrieve more data.
However, we can do one better and cache that operation by using {\tt findAllCached} instead.
For this call, a flag is passed to {\tt findAllCached} that indicates that only the primary table should be loaded.
Otherwise, all other meta data would be loaded which is exactly what the first {\tt findAllCached} does.
The recursive call constructs a cache key using only the primary key to avoid infinite recursion.

The {\textsf Memcached} operations are performed using {\tt getMulti} to minimize the network overhead involved in doing multiple sequential {\textsf Memcached} {\tt get}s.
All cached objects are retrieved with one {\tt getMulti} call and the uncached objects are retrieved with one database query.
The uncached objects are then cached for later use.

% TODO: Indicate what functionality existed previously and what I added (overview of cache APIs).

\subsection{Caching HTML}
We also cache fragments of HTML because template code can be fairly slow and unperformant.
To help facilitate this, we implemented a few different caching functions for use in HTML templates.

The first set of functions is {\tt cacheStart} and {\tt cacheEnd} which are used for caching a single block of HTML.
{\tt cacheStart} accepts either an object that has a {\tt cacheKey} method (e.g. an ORM object) or a string cache key.
Another string is supplied that is added to the cache key to differentiate this fragment of HTML from other fragments constructed from the same object.
If the HTML was found in the cache it is output to {\tt stdout} and {\tt cacheStart} returns {\tt true}.
Otherwise output buffering is started to capture the HTML that is generated when running the template code and {\tt cacheStart} returns {\tt false}.
A call to {\tt cacheEnd} signals the end of the cached HTML section.
This function closes output buffering, gets the contents of the buffer, caches it, and finally outputs the HTML to {\tt stdout}.
A basic example is shown in Figure ~\ref{fig:cacheStartExample}.

% TODO: Make code single spaced.
\begin{figure}[h]
\begin{minted}{php}
<? if (!cacheStart($ormObject, 'view-all', CACHE_FOREVER)): ?>
   <? /* HTML generation */ ?>
<? cacheEnd(); endif; ?>
\end{minted}
\caption{Example usage of cacheStart and cacheEnd.}
\label{fig:cacheStartExample}
\end{figure}

The second caching function we wrote is {\tt batchCache} which is designed for caching lots of fragments of HTML.
The most common use case for it is rendering a list of objects one after the other e.g. search results.
{\tt batchCache} is better than {\tt cacheStart} and {\tt cacheEnd} for this circumstance because of the network overhead for sequential {\textsf Memcached} calls.
This is especially true for fragments of HTML whose generation time is close to or less than the average time of a {\textsf Memcached} {\tt get}.

{\tt batchCache} accepts a mapping of string cache key to any type of data.
The cache keys are used in conjunction with an additional string similar to {\tt cacheStart} to check the cache for HTML.
A function supplied to {\tt batchCache} is called with an array of all the cache keys and data of the missed cache entries.
This allows clients to gather data at the last moment possible because it isn't necessary to get the data at all if the HTML is cached.
Then for each of the resulting objects output buffering is started and the supplied rendering function is called with the object as an argument.
Output buffering is then closed and the resulting HTML is stored in the cache and output to {\tt stdout}.


\chapter{Validation}
\label{validation}
This section was written for 590 when I didn't have a clear topic so this will have to be tweaked once I actually run the experiments.
Other similar papers should be consulted to get a good experimental setup that is accepted in the community.

\section{General Approach}
Validating my work is a fairly straightforward process.
The basic approach will involve gathering various statistics with and without my approach during an experiment based on a repeatable set of input data. %TODO: This sentence kinda sucks.

\section{Measured Values}
Some values that should be increased include cache hit rate and requests per second.
Some values that should be decreased include server load and page response time.
However, merely comparing the two sets of data will not be an accurate picture of the situation.
For example, cache hit rate may be increased as well as server load.
This could be the result of an optimistic prefetching algorithm that caches values before they are requested in the event that they will be requested soon.
The pivotal question is regarding whether or not the increased server load is worth the increased hit rate.

Other criteria, such as ease of use and cache freshness, are harder to measure and validate.
Cache freshness involves measuring how long a stale cache is served after there is a fresh version available.
In particular, some amount of cache staleness is acceptable but the line is blurred especially in the presence of performance gains.

Ease of use is even more difficult to measure.
A caching mechanism that requires a diligent programmer to carefully hand craft the caching algorithm with every feature added is not very easy to use.
On the other hand, a caching mechanism that operates behind the scenes without the programmer's knowledge is very easy to use.
Measuring this will be a very subjective matter.

\section{Experimental Setup}
Developing an experiment to get consistent results is crucial.
I am planning on recording all site traffic during a period of time and replaying it during the experiments.
This ensures that the workload will be the same across all tests.

First an unaltered log of site traffic will be replayed on both systems to emulate real world traffic patterns.
Next this log will be extrapolated to simulate increased site traffic.
Multiple levels of increased site traffic will be run on both systems as a comparison of their merits of scalability.
Scalability tests can be split into two separate groups: general traffic and targetted traffic.
General traffic isn't targetted to any specific pages; requests to all pages are uniformly increased.
This is similar to increasing a site's general popularity.
Targetted traffic on the other hand has increased traffic to a small subset of content.
This simulates special events such as press and social media where a page goes viral for a short amount of time.
Both types of traffic are realistic and are useful measures for the effectiveness of caching strategies.

The contents of the database and {\textsf Memcached} must be the same at the start of the experiments.
The contents of {\textsf Memcached} can be flushed to ensure a consistent repeatable experiment.
However, this setup isn't very realistic because the contents of cache is never completely empty in production and may skew results depending on differences in the caching algorithms.
To mitigate this issue, experiments can be run for a period of time before statistics are collected to make the contents of cache more realistic.

Ensuring the database is in a consistent state is a more difficult matter.
Simply performing a dump of the database won't work because of how long it takes to do so.
Instead, a slave database can be set up to be replicated from the master database.
The instant site traffic starts being recorded the slave will stop replication and the dump will be performed on the slave database.
The contents of the test database will be populated with this dump to guarantee a repeatable experiment.
The only problem with this approach is that populating a database from a MySQL dump may take a significant amount of tme depending on the size of the data.
This will merely increase the time between tests.

All experiments will be run on a duplicate set of machines that mirror the architecture that is used in production.
This ensures that the results gathered will be consistent with how they will perform in production.


% ------------- End main chapters ----------------------

\clearpage
\bibliography{thesis}
\bibliographystyle{plain}
%\addcontentsline{toc}{chapter}{Bibliography}

\end{document}
